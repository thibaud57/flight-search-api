---
title: "Story 4: Crawler & Parser (Proof of Concept Google Flights 1 destination)"
epic: "Epic 2: Google Flights Integration"
story_points: 8
dependencies: ["story-1", "story-2", "story-3"]
date: "2025-11-19"
keywords: ["crawler", "parser", "crawl4ai", "google-flights", "scraping", "stealth-mode", "decodo-proxies", "jsoncssstrategy", "captcha-detection", "pydantic", "proof-of-concept"]
scope: ["specs"]
technologies: ["crawl4ai", "playwright", "pydantic", "decodo", "tenacity"]
---

# üéØ Contexte Business

## Besoin utilisateur

- **Proof of Concept technique** : Valider la faisabilit√© du scraping Google Flights pour une destination unique avant d'impl√©menter la logique multi-destinations complexe
- **D√©risquage early** : Identifier les blocages Google (captchas, rate limiting, anti-bot) d√®s la Story 4 plut√¥t qu'√† la Story 5-6 (√©conomie temps d√©veloppement)
- **Foundation scraping** : √âtablir les patterns r√©utilisables (CrawlerService + FlightParser) pour toutes les stories suivantes (5, 6, 7)
- **Validation stack technique** : Prouver que Crawl4AI + Decodo Proxies + JsonCssExtractionStrategy suffisent pour extraire des donn√©es structur√©es sans LLM

## Contraintes m√©tier

- **Anti-d√©tection Google Flights** : Google utilise Cloudflare/DataDome pour d√©tecter et bloquer les bots (n√©cessite stealth mode + proxies r√©sidentiels Decodo)
- **Captcha detection MVP** : Phase MVP = d√©tection uniquement (logging + retry avec rotation IP), pas de r√©solution automatique (2Captcha r√©serv√© Phase 7 optionnelle si taux blocage >5%)
- **Bandwidth Decodo limit√©** : Proxies r√©sidentiels factur√©s au GB ($2.60-3.50/GB selon volume), minimiser le nombre de requ√™tes et taille HTML t√©l√©charg√©
- **Pas de Database** : R√©sultats en m√©moire uniquement (pas de persistence), focus sur extraction et transformation donn√©es
- **Structure HTML Google Flights non document√©e** : S√©lecteurs CSS peuvent changer sans pr√©avis, n√©cessite monitoring et robustesse parsing

## Valeur business

- ‚úÖ **Validation technique early** : Confirme faisabilit√© scraping Google Flights avant investissement stories multi-destinations (d√©sengagement rapide si bloqu√©)
- ‚úÖ **Feedback qualit√© donn√©es** : Valide que les champs extraits (prix, compagnie, horaires, dur√©e, escales) sont suffisants et correctement format√©s pour ranking futur
- ‚úÖ **Foundation services r√©utilisables** : CrawlerService et FlightParser deviennent les building blocks des stories 5-6 (gain v√©locit√© 30-40%)
- ‚úÖ **M√©triques observables** : √âtablit baseline de performance (taux succ√®s crawl, temps r√©ponse, taux captcha d√©tect√©) pour optimisation continue
- ‚úÖ **R√©duction risque budget** : Valide co√ªts bandwidth Decodo r√©els vs estimations avant scaling multi-destinations

## M√©triques succ√®s

- **Taux succ√®s crawl** : ‚â•85% de requ√™tes Google Flights retournent HTML valide (status 200, pas de captcha)
- **Temps r√©ponse crawl** : ‚â§10 secondes par URL Google Flights (P95 percentile, incluant retry logic)
- **Taux captcha d√©tect√©** : ‚â§5% de requ√™tes bloqu√©es par reCAPTCHA/hCaptcha (target MVP)
- **Taux parsing r√©ussi** : ‚â•95% de HTML valides pars√©s avec succ√®s (minimum 5 vols extraits par recherche)
- **Qualit√© extraction** : 100% des champs obligatoires (price, airline, departure_time, arrival_time, duration) pr√©sents et valides selon sch√©ma Pydantic
- **Coverage tests** : ‚â•80% sur CrawlerService et FlightParser (unitaires + int√©gration)

---

# üìã Sp√©cifications Techniques

## 1. CrawlerService

**R√¥le** : Orchestrer le crawling Google Flights avec Crawl4AI, g√©rer le stealth mode, la rotation de proxies Decodo, et la d√©tection de captchas avec retry logic.

**Interface** :
```python
from typing import Dict, Any
from crawl4ai import CrawlResult

class CrawlerService:
    """Service de crawling Google Flights avec stealth mode et proxy rotation."""

    async def crawl_google_flights(
        self,
        url: str,
        *,
        use_proxy: bool = True,
        max_retries: int = 3
    ) -> CrawlResult:
        """
        Crawl une URL Google Flights avec retry logic et captcha detection.

        Raises:
            CaptchaDetectedError: Si captcha d√©tect√© apr√®s max_retries
            NetworkError: Si erreur r√©seau persistante apr√®s max_retries
        """
```

**Champs/Param√®tres** :

| Champ | Type | Description | Contraintes |
|-------|------|-------------|-------------|
| `url` | `str` | URL Google Flights compl√®te avec param√®tres query | Format `https://www.google.com/travel/flights?...` |
| `use_proxy` | `bool` | Active rotation proxies Decodo | Default `True`, `False` pour dev local |
| `max_retries` | `int` | Nombre maximum de tentatives avec rotation IP | Default `3`, range `1-5` |

**Comportement** :

- **Crawl nominal** :
  1. Initialise AsyncWebCrawler avec BrowserConfig (stealth mode, proxy Decodo si `use_proxy=True`)
  2. Ex√©cute `crawler.arun(url)` avec timeout 10s
  3. V√©rifie status code 200 et absence de captcha dans HTML
  4. Retourne CrawlResult avec `html`, `cleaned_html`, `success=True`

- **Edge cases** :
  - **Captcha d√©tect√©** : Si HTML contient patterns reCAPTCHA/hCaptcha ‚Üí L√®ve `CaptchaDetectedError` avec URL et proxy utilis√©
  - **Status code 403/429** : Rate limiting Google ‚Üí Retry avec rotation proxy et exponential backoff (4s, 8s, 16s)
  - **Timeout r√©seau** : Si `arun()` timeout apr√®s 10s ‚Üí Retry avec nouveau proxy
  - **Max retries d√©pass√©** : L√®ve `NetworkError` avec d√©tails des tentatives √©chou√©es

- **Erreurs lev√©es** :
  - `CaptchaDetectedError` : H√©rit√© de `Exception`, contient `url`, `proxy_used`, `captcha_type` (recaptcha_v2/v3/hcaptcha)
  - `NetworkError` : H√©rit√© de `Exception`, contient `url`, `status_code`, `attempts`

- **Logging structur√©** :
  - INFO : D√©but crawl avec URL et proxy utilis√©
  - WARNING : Captcha d√©tect√© (tentative N/max_retries) + rotation proxy
  - ERROR : Max retries atteint, crawl √©chou√©
  - DEBUG : HTML size, temps r√©ponse, status code

---

## 2. FlightParser

**R√¥le** : Extraire les donn√©es structur√©es de vols depuis le HTML Google Flights via JsonCssExtractionStrategy (sans LLM), valider avec Pydantic, et retourner une liste de mod√®les Flight.

**Interface** :
```python
from typing import List
from app.models.response import Flight

class FlightParser:
    """Parser de vols Google Flights via JsonCssExtractionStrategy."""

    def parse(self, html: str) -> List[Flight]:
        """
        Extrait les vols depuis HTML Google Flights.

        Raises:
            ParsingError: Si aucun vol extrait ou HTML invalide
            ValidationError: Si validation Pydantic √©choue
        """
```

**Schema CSS Extraction** :

```json
{
  "name": "Google Flights Extractor",
  "baseSelector": ".pIav2d",
  "fields": [
    {
      "name": "price",
      "selector": ".FpEdX span:first-child",
      "type": "text"
    },
    {
      "name": "airline",
      "selector": ".sSHqwe",
      "type": "text"
    },
    {
      "name": "departure_time",
      "selector": ".vmXl time",
      "type": "attribute",
      "attribute": "datetime"
    },
    {
      "name": "arrival_time",
      "selector": ".XWcVob time",
      "type": "attribute",
      "attribute": "datetime"
    },
    {
      "name": "duration",
      "selector": ".gvkrdb",
      "type": "text"
    },
    {
      "name": "stops",
      "selector": ".BbR8Ec .ogfYpf",
      "type": "text"
    },
    {
      "name": "departure_airport",
      "selector": ".G2WY5c .sSHqwe:first-child",
      "type": "text"
    },
    {
      "name": "arrival_airport",
      "selector": ".G2WY5c .sSHqwe:last-child",
      "type": "text"
    }
  ]
}
```

**Champs/Param√®tres** :

| Champ | Type | Description | Contraintes |
|-------|------|-------------|-------------|
| `html` | `str` | HTML brut Google Flights | Non vide, min_length > 1000 caract√®res |
| **Retour** | `List[Flight]` | Liste vols extraits et valid√©s | Minimum 1 vol, maximum 50 vols |

**Comportement** :

- **Extraction nominale** :
  1. Instancie JsonCssExtractionStrategy avec schema CSS ci-dessus
  2. Applique strat√©gie sur HTML avec `extraction_strategy.extract(html)`
  3. Transforme chaque √©l√©ment extrait en mod√®le Flight via Pydantic
  4. Valide automatiquement champs obligatoires (price, airline, departure_time, arrival_time)
  5. Retourne liste de Flight valid√©s

- **Edge cases** :
  - **Champs manquants** : Si prix/compagnie/horaires absents dans HTML ‚Üí Skip vol (log WARNING) et continue parsing vols suivants
  - **Prix invalide** : Si prix contient caract√®res non num√©riques (ex: "N/A") ‚Üí Skip vol
  - **HTML malformed** : Si baseSelector `.pIav2d` ne matche aucun √©l√©ment ‚Üí L√®ve `ParsingError("No flights found in HTML")`
  - **Liste vide apr√®s parsing** : Si aucun vol valide extrait ‚Üí L√®ve `ParsingError("Zero valid flights extracted")`

- **Validation Pydantic** :
  - `price` : `float`, valeur > 0
  - `airline` : `str`, min_length=2, max_length=100
  - `departure_time` : `datetime`, format ISO 8601 strict
  - `arrival_time` : `datetime`, format ISO 8601, apr√®s departure_time
  - `duration` : `str`, format "Xh Ymin" (ex: "10h 30min")
  - `stops` : `int | None`, valeur ‚â•0, None si "Non-stop"

- **Erreurs lev√©es** :
  - `ParsingError` : H√©rit√© de `Exception`, contient `html_size`, `flights_found`
  - `ValidationError` : Pydantic standard, contient d√©tails champs invalides

---

## 3. Mod√®le Flight (Pydantic)

**R√¥le** : Repr√©senter un vol extrait avec validation automatique des types et contraintes m√©tier.

**Interface** :
```python
from pydantic import BaseModel, Field, field_validator
from datetime import datetime

class Flight(BaseModel):
    """Mod√®le Pydantic d'un vol extrait depuis Google Flights."""

    price: float = Field(..., gt=0, description="Prix en euros")
    airline: str = Field(..., min_length=2, max_length=100)
    departure_time: datetime
    arrival_time: datetime
    duration: str = Field(..., pattern=r"^\d+h \d+min$")
    stops: int | None = Field(None, ge=0)
    departure_airport: str | None = Field(None, max_length=10)
    arrival_airport: str | None = Field(None, max_length=10)

    @field_validator('arrival_time', mode='after')
    @classmethod
    def arrival_after_departure(cls, v: datetime, info) -> datetime:
        """Valide que arrival_time > departure_time."""
```

**Validations** :
- `field_validator` : V√©rifie coh√©rence temporelle (arrival_time apr√®s departure_time)
- `pattern` : Format dur√©e strict "Xh Ymin" via regex
- `gt=0` : Prix strictement positif

---

# üß™ Tests

## Tests unitaires (TDD)

**Format recommand√© : AAA (Arrange/Act/Assert)**

### CrawlerService (10 tests)

| # | Nom test | Sc√©nario | Input | Output attendu | V√©rification |
|---|----------|----------|-------|----------------|--------------|
| 1 | `test_crawl_success_without_proxy` | Crawl r√©ussi sans proxy (dev mode) | `url="https://google.com/travel/flights?..."`, `use_proxy=False` | `result.success == True`, `result.html` non vide | V√©rifie comportement nominal sans proxy |
| 2 | `test_crawl_success_with_proxy` | Crawl r√©ussi avec proxy Decodo | `url=URL_GOOGLE_FLIGHTS`, `use_proxy=True` | `result.success == True`, proxy utilis√© logg√© | V√©rifie configuration proxy Decodo |
| 3 | `test_crawl_recaptcha_v2_detection` | HTML contient reCAPTCHA v2 | Mock HTML avec `<div class="g-recaptcha">` | L√®ve `CaptchaDetectedError`, `captcha_type="recaptcha_v2"` | V√©rifie d√©tection pattern reCAPTCHA |
| 4 | `test_crawl_hcaptcha_detection` | HTML contient hCaptcha | Mock HTML avec `<div class="h-captcha">` | L√®ve `CaptchaDetectedError`, `captcha_type="hcaptcha"` | V√©rifie d√©tection pattern hCaptcha |
| 5 | `test_crawl_retry_on_captcha` | Captcha d√©tect√©, retry avec rotation proxy | 1√®re tentative captcha, 2√®me succ√®s | `result.success == True`, 2 proxies diff√©rents utilis√©s | V√©rifie retry logic avec rotation IP |
| 6 | `test_crawl_max_retries_exceeded` | Max retries atteint, tous captcha | 3 tentatives, toutes avec captcha | L√®ve `CaptchaDetectedError` final | V√©rifie abandon apr√®s max_retries |
| 7 | `test_crawl_network_timeout` | Timeout r√©seau AsyncWebCrawler | Mock `arun()` timeout apr√®s 10s | L√®ve `NetworkError`, `status_code=None` | V√©rifie gestion timeout |
| 8 | `test_crawl_status_403_retry` | Status code 403 (rate limiting) | Mock response status 403 | Retry avec exponential backoff 4s, 8s, 16s | V√©rifie retry logic sur 403 |
| 9 | `test_crawl_stealth_mode_enabled` | BrowserConfig avec stealth mode actif | `enable_stealth=True` dans config | `result.success == True`, stealth mode logg√© | V√©rifie activation stealth mode |
| 10 | `test_crawl_structured_logging` | Logging structur√© avec contexte | Crawl avec URL et proxy | Logs contiennent `url`, `proxy_used`, `status_code`, `html_size` | V√©rifie qualit√© logging JSON |

### FlightParser (10 tests)

| # | Nom test | Sc√©nario | Input | Output attendu | V√©rification |
|---|----------|----------|-------|----------------|--------------|
| 1 | `test_parse_valid_html_multiple_flights` | HTML valide avec 10 vols | Mock HTML avec 10 `.pIav2d` valides | `len(flights) == 10`, tous Flight valides | V√©rifie extraction nominale |
| 2 | `test_parse_flight_all_fields_present` | Vol avec tous champs renseign√©s | HTML avec price, airline, times, duration, stops, airports | `flight.price > 0`, tous champs non None | V√©rifie mapping complet champs |
| 3 | `test_parse_missing_price` | Vol sans prix | HTML avec airline mais sans `.FpEdX` | Vol skipp√©, log WARNING | V√©rifie robustesse champ obligatoire manquant |
| 4 | `test_parse_invalid_price_format` | Prix non num√©rique | HTML avec price="N/A" | Vol skipp√©, log WARNING | V√©rifie validation format prix |
| 5 | `test_parse_missing_airline` | Vol sans compagnie | HTML avec price mais sans `.sSHqwe` | Vol skipp√©, log WARNING | V√©rifie robustesse champ obligatoire manquant |
| 6 | `test_parse_invalid_datetime_format` | Horaire invalide | HTML avec `datetime="invalid"` | L√®ve `ValidationError` Pydantic | V√©rifie validation datetime strict |
| 7 | `test_parse_arrival_before_departure` | arrival_time < departure_time | HTML avec horaires incoh√©rents | L√®ve `ValidationError` via `field_validator` | V√©rifie validation coh√©rence temporelle |
| 8 | `test_parse_no_flights_found` | HTML sans `.pIav2d` | HTML Google Flights vide ou malformed | L√®ve `ParsingError("No flights found")` | V√©rifie gestion HTML invalide |
| 9 | `test_parse_stops_nonstop` | Vol direct | HTML avec "Non-stop" | `flight.stops == 0` | V√©rifie parsing "Non-stop" ‚Üí int 0 |
| 10 | `test_parse_stops_multiple` | Vol avec escales | HTML avec "2 stops" | `flight.stops == 2` | V√©rifie extraction nombre escales |

**Total tests unitaires** : 10 + 10 = 20 tests

---

## Tests int√©gration

**Format recommand√© : Given/When/Then (BDD)**

| # | Nom test | Pr√©requis (Given) | Action (When) | R√©sultat attendu (Then) |
|---|----------|-------------------|---------------|-------------------------|
| 1 | `test_integration_crawl_and_parse_success` | Mock AsyncWebCrawler avec HTML Google Flights valide (10 vols) | Crawl URL ‚Üí Parse HTML | `len(flights) == 10`, tous Flight valid√©s Pydantic, pas d'exception |
| 2 | `test_integration_crawl_captcha_retry_parse_success` | Mock AsyncWebCrawler : 1√®re tentative captcha, 2√®me HTML valide | Crawl avec retry ‚Üí Parse HTML 2√®me tentative | `len(flights) >= 5`, 2 proxies utilis√©s, logs WARNING captcha puis SUCCESS |
| 3 | `test_integration_crawl_max_retries_no_parse` | Mock AsyncWebCrawler : toutes tentatives retournent captcha | Crawl avec max_retries=3 | L√®ve `CaptchaDetectedError`, aucun parsing tent√©, 3 proxies diff√©rents utilis√©s |
| 4 | `test_integration_crawl_success_parse_zero_flights` | Mock AsyncWebCrawler avec HTML Google Flights vide (aucun `.pIav2d`) | Crawl URL ‚Üí Parse HTML vide | L√®ve `ParsingError("Zero valid flights extracted")`, crawl success mais parsing fail |

**Total tests int√©gration** : 4 tests

---

**TOTAL TESTS** : 20 unitaires + 4 int√©gration = **24 tests**

---

## Exemples JSON

**Exemple 1 : HTML Google Flights valide (extrait)**
```json
{
  "html_snippet": "<div class='pIav2d'><div class='FpEdX'><span>1250 ‚Ç¨</span></div><div class='sSHqwe'>Air France</div><div class='vmXl'><time datetime='2025-06-01T10:30:00'>10:30</time></div><div class='XWcVob'><time datetime='2025-06-01T14:45:00'>14:45</time></div><div class='gvkrdb'>10h 15min</div><div class='BbR8Ec'><div class='ogfYpf'>1 stop</div></div></div>",
  "description": "HTML d'un vol Air France Paris-Tokyo avec 1 escale"
}
```

**Exemple 2 : Flight extrait et valid√©**
```json
{
  "price": 1250.0,
  "airline": "Air France",
  "departure_time": "2025-06-01T10:30:00Z",
  "arrival_time": "2025-06-01T14:45:00Z",
  "duration": "10h 15min",
  "stops": 1,
  "departure_airport": "CDG",
  "arrival_airport": "NRT"
}
```

**Exemple 3 : Erreur CaptchaDetectedError**
```json
{
  "error": "CaptchaDetectedError",
  "message": "reCAPTCHA v2 detected after 3 retries",
  "details": {
    "url": "https://www.google.com/travel/flights?departure_id=CDG&arrival_id=NRT&outbound_date=2025-06-01",
    "proxy_used": "customer-abc123-country-fr:password @ gate.decodo.com:7000",
    "captcha_type": "recaptcha_v2",
    "attempts": 3,
    "proxies_tried": ["proxy1", "proxy2", "proxy3"]
  }
}
```

**Exemple 4 : Log structur√© crawl success**
```json
{
  "timestamp": "2025-11-19T10:30:45Z",
  "level": "INFO",
  "service": "CrawlerService",
  "message": "Crawl successful",
  "context": {
    "url": "https://www.google.com/travel/flights?departure_id=CDG&arrival_id=NRT&outbound_date=2025-06-01",
    "proxy_used": "customer-abc123-country-fr:password @ gate.decodo.com:7000",
    "status_code": 200,
    "html_size": 245678,
    "response_time_ms": 2345,
    "stealth_mode": true,
    "retry_attempt": 0
  }
}
```

**Exemple 5 : ParsingError (aucun vol extrait)**
```json
{
  "error": "ParsingError",
  "message": "Zero valid flights extracted from HTML",
  "details": {
    "html_size": 123456,
    "base_selector_matches": 0,
    "reason": "No .pIav2d elements found in HTML"
  }
}
```

---

# ‚úÖ Crit√®res d'acceptation

## Crit√®res fonctionnels

1. **Crawl Google Flights 1 destination r√©ussi** : CrawlerService crawle avec succ√®s une URL Google Flights aller-simple (Paris‚ÜíTokyo) et retourne CrawlResult avec HTML valide (status 200, ‚â•100KB HTML)

2. **Stealth mode actif** : BrowserConfig configur√© avec `enable_stealth=True`, v√©rifi√© dans logs structur√©s (champ `stealth_mode: true`)

3. **Captcha d√©tect√© et logg√©** : Si reCAPTCHA/hCaptcha pr√©sent dans HTML ‚Üí CaptchaDetectedError lev√©e avec `captcha_type` et `proxy_used` logg√©s en WARNING

4. **Retry logic avec rotation IP** : En cas de captcha/403/timeout ‚Üí retry avec exponential backoff (4s, 8s, 16s) et rotation proxies Decodo (3 proxies diff√©rents utilis√©s max)

5. **Parsing extrait minimum 5 vols** : FlightParser parse HTML Google Flights et retourne liste ‚â•5 Flight valid√©s Pydantic (champs obligatoires pr√©sents)

6. **Format Flight valide** : Chaque Flight contient `price > 0`, `airline` (2-100 caract√®res), `departure_time` et `arrival_time` (datetime ISO 8601), `duration` (format "Xh Ymin"), `stops` (int ‚â•0 ou None)

7. **Gestion champs manquants** : Si prix/compagnie/horaires absents ‚Üí vol skipp√© avec log WARNING, parsing continue pour vols suivants (pas d'exception bloquante)

8. **Erreurs explicites** : CaptchaDetectedError et ParsingError lev√©es avec messages descriptifs et contexte (URL, proxy, HTML size, attempts)

## Crit√®res techniques

9. **Type hints PEP 695** : Toutes signatures CrawlerService, FlightParser, Flight annot√©es avec type hints modernes (`list[Flight]`, `str | None`, `async def`)

10. **Async/Await coh√©rent** : CrawlerService.crawl_google_flights async, utilise `async with AsyncWebCrawler`, `await crawler.arun()`, pas de blocking IO

11. **Retry logic Tenacity** : D√©corateur `@retry` configur√© avec `stop_after_attempt(3)`, `wait_exponential(multiplier=2, min=4, max=60)`, `retry_if_exception_type(CaptchaDetectedError | NetworkError)`

12. **Pydantic v2** : Flight utilise `BaseModel`, `Field(...)` avec contraintes, `field_validator` pour validation cross-champs, `model_config` avec `ConfigDict(frozen=False, extra='forbid')`

13. **JsonCssExtractionStrategy** : Schema CSS d√©fini avec `baseSelector`, `fields` (8 champs minimum), types `text`/`attribute`, pas de LLM

14. **Decodo Proxies configuration** : Format auth `customer-{api_key}-country-fr:password`, endpoint `gate.decodo.com:7000`, rotation cyclique avec `itertools.cycle(proxy_pool)`

15. **Logging structur√© JSON** : Tous logs avec `extra={}` contenant contexte m√©tier (`url`, `proxy_used`, `status_code`, `html_size`, `response_time_ms`, `stealth_mode`, `retry_attempt`)

16. **Exceptions custom** : CaptchaDetectedError et ParsingError h√©ritent de Exception, contiennent attributs contextuels (`url`, `proxy_used`, `captcha_type`, `html_size`, `flights_found`)

## Crit√®res qualit√©

17. **Coverage ‚â•80%** : Tests unitaires + int√©gration couvrent minimum 80% du code de CrawlerService et FlightParser (pytest-cov)

18. **24 tests passent** : 20 tests unitaires (10 CrawlerService + 10 FlightParser) + 4 tests int√©gration tous verts (pytest -v)

19. **Ruff + Mypy passent** : `ruff check .` et `ruff format .` sans erreur, `mypy app/` strict mode sans erreur type

20. **Tests TDD format AAA** : Tests unitaires suivent strictement Arrange/Act/Assert, tableaux specs compl√©t√©s avec 6 colonnes (N¬∞, Nom, Sc√©nario, Input, Output, V√©rification)

21. **Tests int√©gration format Given/When/Then** : Tests int√©gration suivent BDD avec 5 colonnes (N¬∞, Nom, Pr√©requis, Action, R√©sultat), mocks AsyncWebCrawler configur√©s

22. **Docstrings 1 ligne** : CrawlerService et FlightParser avec docstring descriptive, m√©thodes principales document√©es, focus POURQUOI pas QUOI

23. **Aucun code production dans specs** : Ce document contient uniquement signatures, tableaux tests, descriptions comportements, exemples JSON (pas d'impl√©mentation compl√®te de m√©thodes)

24. **Commits conventional** : Story 4 committ√©e avec message `docs(specs): add story 4 specifications` conforme Conventional Commits

---

**üí° Note** : Cette story est un Proof of Concept (8 story points). Les 24 crit√®res couvrent faisabilit√© technique (crawl + parsing 1 destination), robustesse (captcha detection, retry logic), qualit√© (coverage, types, tests), et foundation r√©utilisable pour stories 5-6 (multi-destinations, search orchestration).
