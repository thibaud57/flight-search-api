---
title: "Story 4: Crawler & Parser (Proof of Concept Google Flights 1 destination)"
epic: "Epic 2: Google Flights Integration"
story_points: 8
dependencies: ["epic-1/story-1", "epic-1/story-2", "epic-1/story-3"]
date: "2025-19-11"
keywords: ["crawler", "parser", "crawl4ai", "google-flights", "scraping", "stealth-mode", "jsoncssstrategy", "captcha-detection", "pydantic", "proof-of-concept"]
scope: ["specs"]
technologies: ["crawl4ai", "playwright", "pydantic"]
---

# üéØ Contexte Business

## Besoin utilisateur

- **Proof of Concept technique** : Valider la faisabilit√© du scraping Google Flights pour une destination unique avant d'impl√©menter la logique multi-destinations complexe
- **D√©risquage early** : Identifier les blocages Google (captchas, rate limiting, anti-bot) d√®s la Story 4 plut√¥t qu'aux stories 5 et 6 (√©conomie temps d√©veloppement)
- **Foundation scraping** : √âtablir les patterns r√©utilisables (CrawlerService + FlightParser) pour toutes les stories suivantes (5, 6, 7)
- **Validation stack technique** : Prouver que Crawl4AI + JsonCssExtractionStrategy suffisent pour extraire des donn√©es structur√©es sans LLM (POC dev local)

## Contraintes m√©tier

- **Anti-d√©tection Google Flights** : Google utilise Cloudflare/DataDome pour d√©tecter et bloquer les bots (stealth mode Crawl4AI activ√©, proxies ajout√©s Story 5)
- **Captcha detection MVP** : Phase MVP = d√©tection uniquement (logging), pas de r√©solution automatique (rotation proxies Story 5, retry Story 7, 2Captcha Phase 7 optionnel si taux blocage >5%)
- **Bandwidth Google Flights** : Minimiser nombre de requ√™tes et taille HTML t√©l√©charg√© pour √©viter rate limiting (POC = tests dev local limit√©s)
- **Pas de Database** : R√©sultats en m√©moire uniquement (pas de persistence), focus sur extraction et transformation donn√©es
- **Structure HTML Google Flights non document√©e** : S√©lecteurs CSS peuvent changer sans pr√©avis, n√©cessite monitoring et robustesse parsing

## Valeur business

- ‚úÖ **Validation technique early** : Confirme faisabilit√© scraping Google Flights avant investissement stories multi-destinations (d√©sengagement rapide si bloqu√©)
- ‚úÖ **Feedback qualit√© donn√©es** : Valide que les champs extraits (prix, compagnie, horaires, dur√©e, escales) sont suffisants et correctement format√©s pour ranking futur
- ‚úÖ **Foundation services r√©utilisables** : CrawlerService et FlightParser deviennent les building blocks des stories 5-6 (gain v√©locit√© 30-40%)
- ‚úÖ **M√©triques observables** : √âtablit baseline de performance (taux succ√®s crawl, temps r√©ponse, taux captcha d√©tect√©) pour optimisation continue
- ‚úÖ **R√©duction risque budget** : Valide co√ªts bandwidth Decodo r√©els vs estimations avant scaling multi-destinations

## M√©triques succ√®s

- **Taux succ√®s crawl** : ‚â•85% de requ√™tes Google Flights retournent HTML valide (status 200, pas de captcha)
- **Temps r√©ponse crawl** : ‚â§10 secondes par URL Google Flights (P95 percentile, POC dev local)
- **Taux captcha d√©tect√©** : ‚â§5% de requ√™tes bloqu√©es par reCAPTCHA/hCaptcha (target MVP)
- **Taux parsing r√©ussi** : ‚â•95% de HTML valides pars√©s avec succ√®s (minimum 5 vols extraits par recherche)
- **Qualit√© extraction** : 100% des champs obligatoires (price, airline, departure_time, arrival_time, duration) pr√©sents et valides selon sch√©ma Pydantic
- **Coverage tests** : ‚â•80% sur CrawlerService et FlightParser (unitaires + int√©gration)

---

# üìã Sp√©cifications Techniques

## 1. CrawlerService

**R√¥le** : Orchestrer le crawling Google Flights avec Crawl4AI, g√©rer session capture, browser fingerprinting, stealth mode, proxy rotation (Story 5) et d√©tection captchas.

**Interface** :
```python
@dataclass
class CrawlResult:
    """Resultat d'un crawl."""

    success: bool
    html: str
    status_code: int | None = None


class CrawlerService:
    """Service de crawling Google Flights avec stealth mode et proxy rotation."""

    def __init__(self, proxy_service: ProxyService | None = None):
        """Initialise service avec ProxyService optionnel (Story 5)."""

    async def get_google_session(
        self,
        url: str = "https://www.google.com/travel/flights",
        *,
        use_proxy: bool = True,
    ) -> None:
        """
        Capture session Google (headers + cookies) via Crawl4AI avec persistence.

        Capte cookies Google l√©gitimes et accepte automatiquement popup RGPD.
        Les cookies sont r√©utilis√©s dans crawl_google_flights().
        """

    async def crawl_google_flights(
        self,
        url: str,
        *,
        use_proxy: bool = True
    ) -> CrawlResult:
        """
        Crawl une URL Google Flights avec proxy rotation et browser fingerprinting.

        Raises:
            CaptchaDetectedError: Si captcha d√©tect√©
            NetworkError: Si erreur r√©seau ou timeout
        """
```

**Champs/Param√®tres** :

| Champ | Type | Description | Contraintes |
|-------|------|-------------|-------------|
| `url` | `str` | URL Google Flights compl√®te avec param√®tres query | Format `https://www.google.com/travel/flights?...` |
| `use_proxy` | `bool` | Activer proxy rotation (Story 5) | Default `True`, requiert ProxyService inject√© |

**Comportement** :

### M√©thode get_google_session()

**Nouvelle m√©thode (non sp√©cifi√©e POC initial)** : Capture session Google avant crawls pour am√©liorer taux succ√®s.

- **Session capture nominale** :
  1. Obtient proxy depuis ProxyService si `use_proxy=True`
  2. Construit BrowserConfig avec base configuration (headers Chrome, stealth args)
  3. Initialise AsyncWebCrawler avec hooks Playwright
  4. Configure hooks :
     - `after_goto` : Auto-click popup RGPD "Tout accepter" (wait_for_selector 1s)
     - `before_return_html` : Capture cookies via `context.cookies()`
  5. Execute `crawler.arun()` sur URL Google Flights (timeout 50s)
  6. Stocke cookies captur√©s pour r√©utilisation future
  7. Logger cookies_captured count

- **Edge cases** :
  - **Popup RGPD absent** : Timeout wait_for_selector 1s ‚Üí Log WARNING, continue sans click
  - **Timeout session capture** : 50s global timeout ‚Üí L√®ve `NetworkError`
  - **Status 403/429** : L√®ve `NetworkError` avec status_code

### M√©thode crawl_google_flights()

- **Crawl nominal avec fingerprinting** :
  1. Obtient proxy depuis ProxyService si `use_proxy=True` (rotation automatique)
  2. Construit BrowserConfig avec fingerprint complet :
     - Headers statiques Chrome 142 (27 headers incluant Client Hints sec-ch-ua)
     - Cookies captur√©s depuis get_google_session()
     - Proxy config si activ√©
     - Viewport 1920√ó1080
  3. Initialise AsyncWebCrawler avec BrowserConfig
  4. Configure CrawlerRunConfig :
     - `wait_for="css:.pIav2d"` (attendre cartes vols)
     - `page_timeout` : Configurable via Settings (30s)
     - `delay_before_return_html` : Configurable via Settings (2s)
  5. Ex√©cute `crawler.arun(url)` avec timeout global 50s
  6. V√©rifie status code 200 et absence de captcha dans HTML
  7. Retourne CrawlResult avec `success=True`, `html`, `status_code`

- **Edge cases** :
  - **Captcha d√©tect√©** : Si HTML contient patterns reCAPTCHA/hCaptcha ‚Üí L√®ve `CaptchaDetectedError` avec URL et type captcha
  - **Status code 403/429** : Rate limiting Google ‚Üí L√®ve `NetworkError` avec status code
  - **Timeout r√©seau** : Si `arun()` timeout apr√®s 50s ‚Üí L√®ve `NetworkError` avec status_code=None
  - **Proxy d√©sactiv√©** : Si `use_proxy=False` ou ProxyService=None ‚Üí BrowserConfig sans proxy (mode direct)

- **Erreurs lev√©es** :
  - `CaptchaDetectedError` : H√©rit√© de `Exception`, contient `url`, `captcha_type` (recaptcha/hcaptcha)
  - `NetworkError` : H√©rit√© de `Exception`, contient `url`, `status_code`

- **Logging structur√©** :
  - INFO : D√©but crawl avec URL, proxy_host, proxy_country
  - INFO : Crawl successful avec status_code, html_size, response_time_ms, proxy_host
  - WARNING : Captcha detected avec captcha_type
  - ERROR : Crawl failed avec status_code, proxy_host

### Browser Fingerprinting (utils/browser_fingerprint.py)

**Nouveau module (non sp√©cifi√© POC initial)** : Anti-d√©tection avanc√©.

- **Headers statiques Chrome 142** :
  - 27 headers HTTP √©prouv√©s incluant :
    - Client Hints complets : `sec-ch-ua`, `sec-ch-ua-mobile`, `sec-ch-ua-platform`
    - Accept headers : `text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8`
    - User-Agent : Chrome 142.0.6367.62 Windows NT 10.0
    - Referer, Origin, Accept-Language, Accept-Encoding

- **Stealth args Chromium** :
  - `--disable-blink-features=AutomationControlled` (masque automation)
  - `--disable-webrtc` (√©vite IP leak)
  - `--disable-dev-shm-usage` (stabilit√© Docker)
  - `--no-sandbox` (si n√©cessaire environnement)

- **Viewport** : 1920√ó1080 (r√©solution courante desktop)

### Hooks Playwright

**Nouveaux hooks (non sp√©cifi√©s POC initial)** :

| Hook | Trigger | R√¥le | Impl√©mentation |
|------|---------|------|----------------|
| `_after_goto_hook` | Apr√®s navigation page | Auto-click popup RGPD | Attente s√©lecteur 1s ‚Üí Click button acceptation |
| `_extract_cookies_hook` | Avant return HTML | Capture cookies session | Extraction via contexte navigateur ‚Üí Stockage interne |

**Justification hooks** :
- Popup RGPD bloque scraping si non accept√© ‚Üí Auto-click am√©liore taux succ√®s
- Cookies Google l√©gitimes r√©duisent d√©tection bot ‚Üí Session persistence

### Timeouts Configurables

**Nouveaux settings (non sp√©cifi√©s POC initial)** :

| Setting | Default | Description | Changement vs POC |
|---------|---------|-------------|-------------------|
| `crawl_global_timeout_s` | 50s | Timeout total asyncio.wait_for | POC = 10s fixe |
| `crawl_page_timeout_ms` | 30000ms | Timeout page load Playwright | POC = non configur√© |
| `crawl_delay_s` | 2s | D√©lai avant return HTML | POC = non configur√© |

**Justification** : Timeouts POC 10s trop court pour pages lourdes Google Flights (HTML ~200-500KB + JS loading)

---

## 2. FlightParser

**R√¥le** : Extraire les donn√©es structur√©es de vols depuis le HTML Google Flights via JsonCssExtractionStrategy + parsing aria-label avec regex (sans LLM), valider avec Pydantic, et retourner une liste de mod√®les GoogleFlightDTO.

**Interface** :
```python
class FlightParser:
    """Parser de vols Google Flights avec JsonCssExtractionStrategy + aria-label."""

    def parse(self, html: str) -> list[GoogleFlightDTO]:
        """
        Extrait les vols depuis HTML Google Flights.

        Raises:
            ParsingError: Si aucun vol extrait ou HTML invalide
            ValidationError: Si validation Pydantic √©choue
        """
```

**Strat√©gie d'extraction (aria-label + regex)** :

**Approche actuelle** : Au lieu d'extraire 8 champs CSS s√©par√©s, l'extraction se fait en 2 √©tapes :

1. **√âtape 1 - JsonCssExtractionStrategy** : Extraction d'un seul champ `aria-label` par carte de vol
   - S√©lecteur base : `li.pIav2d` (conteneur de chaque vol)
   - Champ extrait : attribut `aria-label` du `div[aria-label]` enfant

2. **√âtape 2 - Regex patterns** : Parsing de l'aria-label avec 8 regex pour extraire les champs

**Configuration JsonCssExtractionStrategy** :

| Propri√©t√© | Valeur | Description |
|-----------|--------|-------------|
| `name` | "Google Flights Results" | Nom du sch√©ma d'extraction |
| `baseSelector` | `li.pIav2d` | S√©lecteur CSS des cartes vols individuelles |
| **Field 1** | | **Extraction aria-label** |
| `name` | "aria_label" | Nom du champ extrait |
| `selector` | `div[aria-label]` | S√©lecteur CSS de l'√©l√©ment contenant aria-label |
| `type` | "attribute" | Type d'extraction (attribut HTML) |
| `attribute` | "aria-label" | Nom de l'attribut HTML √† extraire |

**Regex patterns pour parsing aria-label** :

| Champ | Pattern regex | Exemple match | Notes |
|-------|--------------|---------------|-------|
| `price` | `(\d+(?:\s?\d+)*)\s*euros` | "1270 euros" ‚Üí 1270.0 | G√®re espaces dans nombres (1 270 euros) |
| `airline` | `avec\s+([^.,]+)` | "avec ANA" ‚Üí "ANA" | Extrait compagnie apr√®s "avec" |
| `departure_time` | `D√©part.*?(\d{1,2}:\d{2})` | "D√©part... 10:30" ‚Üí "10:30" | Format HH:MM |
| `arrival_time` | `arriv√©e.*?(\d{1,2}:\d{2})` | "arriv√©e... 14:45" ‚Üí "14:45" | Format HH:MM |
| `duration` | `Dur√©e totale\s*:\s*(.+?)(?:\.|$)` | "Dur√©e totale : 13 h 40 min" ‚Üí "13 h 40 min" | Texte libre apr√®s "Dur√©e totale" |
| `stops` | `(\d+)\s*escales?` | "1 escale" ‚Üí 1 | Parsing "Vol direct" ‚Üí 0 (cas sp√©cial) |
| `departure_airport` | `D√©part de ([^√†]+) √†` | "D√©part de Paris √†" ‚Üí "Paris" | Nom a√©roport complet |
| `arrival_airport` | `arriv√©e √† ([^√†]+) √†` | "arriv√©e √† Tokyo √†" ‚Üí "Tokyo" | Nom a√©roport complet |

**Exemple aria-label r√©el Google Flights** :

```
"√Ä partir de 1270 euros. D√©part de Paris √† 10:30, arriv√©e √† Tokyo √† 14:45.
Dur√©e totale : 13 h 40 min. 1 escale avec ANA."
```

**Parsing flow complet** :

1. JsonCssExtractionStrategy extrait `aria_label` pour chaque carte vol
2. Pour chaque aria-label :
   - Appliquer 8 regex patterns pour extraire champs
   - Si champs obligatoires manquants (price, airline, departure_time, arrival_time) ‚Üí Skip vol
   - Construire GoogleFlightDTO avec champs extraits
   - Valider automatiquement via Pydantic
3. Retourner liste GoogleFlightDTO valid√©s

**Champs/Param√®tres** :

| Champ | Type | Description | Contraintes |
|-------|------|-------------|-------------|
| `html` | `str` | HTML brut Google Flights | Non vide, min_length > 1000 caract√®res |
| **Retour** | `list[GoogleFlightDTO]` | Liste vols extraits et valid√©s | Minimum 1 vol, maximum 50 vols |

**Comportement** :

- **Extraction nominale** :
  1. Instancie JsonCssExtractionStrategy avec FLIGHT_SCHEMA ci-dessus
  2. Applique strat√©gie sur HTML : `extraction_strategy.extract(url="", html_content=html)`
  3. Pour chaque r√©sultat brut contenant `aria_label` :
     - Parse aria_label avec 8 regex patterns
     - Construit GoogleFlightDTO si champs obligatoires pr√©sents
  4. Retourne liste de GoogleFlightDTO valid√©s

- **Edge cases** :
  - **Aria-label absent** : Si `aria_label` vide ou None ‚Üí Skip vol (log WARNING) et continue parsing vols suivants
  - **Champs manquants** : Si price/airline/departure_time/arrival_time absents dans aria-label ‚Üí Skip vol (log WARNING)
  - **Prix invalide** : Si regex prix ne matche pas ou conversion float √©choue ‚Üí Skip vol
  - **HTML malformed** : Si baseSelector `li.pIav2d` ne matche aucun √©l√©ment ‚Üí L√®ve `ParsingError("No flights found in HTML")`
  - **Liste vide apr√®s parsing** : Si aucun vol valide extrait ‚Üí L√®ve `ParsingError("Zero valid flights extracted")`

- **Validation Pydantic** : Voir section 3 (GoogleFlightDTO)

- **Erreurs lev√©es** :
  - `ParsingError` : H√©rit√© de `Exception`, contient `html_size`, `flights_found`
  - `ValidationError` : Pydantic standard, contient d√©tails champs invalides

**Justification approche aria-label** :
- ‚úÖ **Plus robuste** : aria-label = texte stable pour accessibilit√©, s√©lecteurs CSS Google changent fr√©quemment
- ‚úÖ **Maintenance** : 1 s√©lecteur base + 8 regex patterns vs 8 s√©lecteurs CSS fragiles
- ‚ö†Ô∏è **Limitation** : D√©pend format texte fran√ßais (`euros`, `avec`, `D√©part`), pas multilingue (n√©cessiterait adaptation regex par langue)

---

## 3. Mod√®le GoogleFlightDTO (Pydantic)

**R√¥le** : Repr√©senter un vol extrait avec validation automatique des types et contraintes m√©tier.

**Interface** :
```python
class GoogleFlightDTO(BaseModel):
    """Mod√®le Pydantic d'un vol extrait depuis Google Flights."""

    price: Annotated[float, Field(gt=0)]
    airline: Annotated[str, Field(min_length=2, max_length=100)]
    departure_time: str
    arrival_time: str
    duration: str
    stops: Annotated[int | None, Field(ge=0)] = None
    departure_airport: Annotated[str | None, Field(max_length=200)] = None
    arrival_airport: Annotated[str | None, Field(max_length=200)] = None
```

**Validations Pydantic** :

| Champ | Contrainte | Description |
|-------|-----------|-------------|
| `price` | `> 0` | Prix strictement positif (euros) |
| `airline` | `min_length=2, max_length=100` | Nom compagnie valide |
| `departure_time` | Format `HH:MM` | Heure locale format simple (ex: "10:30") |
| `arrival_time` | Format `HH:MM` | Heure locale format simple (ex: "14:45") |
| `duration` | Texte libre | Format dur√©e variable (ex: "10h 30min", "13 h 40 min") |
| `stops` | `‚â• 0` ou `None` | Nombre escales valide |
| `departure_airport` | `max_length=200` | Nom a√©roport complet ou code (ex: "Paris Charles de Gaulle" ou "CDG") |
| `arrival_airport` | `max_length=200` | Nom a√©roport complet ou code (ex: "Tokyo Narita" ou "NRT") |

**Notes importantes** :
- **Types modifi√©s** : `departure_time` et `arrival_time` sont des `str` (format HH:MM) et non `datetime` car Google Flights retourne uniquement les heures locales sans date ni timezone compl√®tes dans l'aria-label
- **Airport max_length relax√©** : Pass√© de 10 √† 200 caract√®res pour supporter les noms complets d'a√©roports (ex: "Paris Charles de Gaulle") en plus des codes IATA/ICAO
- **Pas de validation cross-champs** : Impossible de valider `arrival_time > departure_time` car les heures sont au format HH:MM sans date (un vol de 23:00 √† 02:00 traverse minuit)

---

# üß™ Tests

## Tests unitaires (TDD)

**Format recommand√© : AAA (Arrange/Act/Assert)**

### CrawlerService (13 tests)

| # | Nom test | Sc√©nario | Input | Output attendu | V√©rification |
|---|----------|----------|-------|----------------|--------------|
| 1 | `test_get_google_session_success` | Session capture r√©ussie | URL Google Flights | Cookies captur√©s stock√©s pour r√©utilisation, logs INFO cookies_captured | V√©rifie m√©thode get_google_session() |
| 2 | `test_get_google_session_auto_click_consent` | Auto-click popup RGPD | Mock page avec button "Tout accepter" | Button cliqu√© automatiquement, sleep 1s, logs INFO | V√©rifie hook _after_goto_hook |
| 3 | `test_get_google_session_no_consent_popup` | Popup RGPD absent | Mock page sans button consent | Timeout 1s, log WARNING, continue sans click, pas d'exception | V√©rifie robustesse edge case popup absent |
| 4 | `test_crawl_success_with_fingerprint` | Crawl r√©ussi avec fingerprinting complet | URL Google Flights | `result.success == True`, `result.html` non vide, BrowserConfig avec 27 headers + cookies | V√©rifie comportement nominal avec fingerprint |
| 5 | `test_crawl_recaptcha_detection` | HTML contient reCAPTCHA | Mock HTML avec pattern `g-recaptcha` | L√®ve `CaptchaDetectedError`, `captcha_type="recaptcha"` | V√©rifie d√©tection pattern reCAPTCHA |
| 6 | `test_crawl_hcaptcha_detection` | HTML contient hCaptcha | Mock HTML avec pattern `h-captcha` | L√®ve `CaptchaDetectedError`, `captcha_type="hcaptcha"` | V√©rifie d√©tection pattern hCaptcha |
| 7 | `test_crawl_network_timeout` | Timeout r√©seau AsyncWebCrawler | Mock `arun()` timeout apr√®s 50s | L√®ve `NetworkError`, `status_code=None` | V√©rifie gestion timeout (50s configur√©) |
| 8 | `test_crawl_status_403` | Status code 403 (rate limiting) | Mock response status 403 | L√®ve `NetworkError`, `status_code=403` | V√©rifie lev√©e erreur sur 403 |
| 9 | `test_crawl_status_429` | Status code 429 (rate limiting) | Mock response status 429 | L√®ve `NetworkError`, `status_code=429` | V√©rifie lev√©e erreur sur 429 |
| 10 | `test_crawl_with_proxy_rotation` | Crawl avec proxy activ√© | `use_proxy=True`, ProxyService mock√© | `proxy_service.get_next_proxy()` appel√©, BrowserConfig avec proxy_config | V√©rifie int√©gration ProxyService (Story 5) |
| 11 | `test_crawl_without_proxy` | Crawl sans proxy | `use_proxy=False` | BrowserConfig sans proxy_config, logs proxy_host="no_proxy" | V√©rifie mode direct sans proxy |
| 12 | `test_crawl_timeouts_configurable` | Timeouts configurables via Settings | Settings avec custom timeouts | CrawlerRunConfig.page_timeout depuis settings, asyncio.wait_for timeout depuis settings | V√©rifie settings.crawler.crawl_*_timeout |
| 13 | `test_crawl_structured_logging` | Logging structur√© avec contexte | Crawl avec URL + proxy | Logs contiennent `url`, `status_code`, `html_size`, `response_time_ms`, `proxy_host`, `proxy_country` | V√©rifie qualit√© logging JSON |

### FlightParser (10 tests)

| # | Nom test | Sc√©nario | Input | Output attendu | V√©rification |
|---|----------|----------|-------|----------------|--------------|
| 1 | `test_parse_valid_html_multiple_flights` | HTML valide avec 10 vols aria-label | Mock HTML avec 10 `li.pIav2d` contenant aria-label valides | `len(flights) == 10`, tous GoogleFlightDTO valides | V√©rifie extraction nominale aria-label |
| 2 | `test_parse_aria_label_all_fields_present` | Aria-label avec tous champs | Aria-label complet : "√Ä partir de 1270 euros... avec ANA..." | `flight.price == 1270.0`, tous champs extraits (airline, times, duration, stops, airports) | V√©rifie parsing regex complet |
| 3 | `test_parse_price_with_spaces` | Prix avec espaces | Aria-label avec "1 270 euros" (espace s√©parateur) | `flight.price == 1270.0` (espace supprim√©) | V√©rifie regex price g√®re espaces |
| 4 | `test_parse_missing_price` | Aria-label sans prix | Aria-label avec airline mais sans pattern "euros" | Vol skipp√©, log WARNING, retour None | V√©rifie robustesse champ obligatoire manquant |
| 5 | `test_parse_invalid_price_format` | Prix non num√©rique | Aria-label avec "N/A euros" (conversion float impossible) | Vol skipp√©, log WARNING, retour None | V√©rifie validation format prix apr√®s extraction |
| 6 | `test_parse_missing_airline` | Aria-label sans compagnie | Aria-label avec price mais sans pattern "avec" | Vol skipp√©, log WARNING, retour None | V√©rifie robustesse champ obligatoire manquant |
| 7 | `test_parse_missing_departure_time` | Horaires manquants | Aria-label sans pattern "D√©part... HH:MM" | Vol skipp√© (champs obligatoires absents), retour None | V√©rifie validation champs obligatoires |
| 8 | `test_parse_no_flights_found` | HTML sans `li.pIav2d` | HTML Google Flights vide ou malformed (aucun baseSelector match) | L√®ve `ParsingError("No flights found in HTML")` | V√©rifie gestion HTML invalide |
| 9 | `test_parse_stops_vol_direct` | Vol direct | Aria-label avec "Vol direct" (texte fran√ßais) | `flight.stops == 0` (cas sp√©cial d√©tect√©) | V√©rifie parsing "Vol direct" ‚Üí int 0 |
| 10 | `test_parse_stops_multiple_escales` | Vol avec escales | Aria-label avec "2 escales" ou "1 escale" | `flight.stops == 2` ou `1` (regex capture nombre) | V√©rifie extraction nombre escales depuis regex |

**Total tests unitaires** : 13 (CrawlerService) + 10 (FlightParser) = **23 tests**

---

## Tests int√©gration

**Format recommand√© : Given/When/Then (BDD)**

| # | Nom test | Pr√©requis (Given) | Action (When) | R√©sultat attendu (Then) |
|---|----------|-------------------|---------------|-------------------------|
| 1 | `test_integration_crawl_and_parse_success` | Mock AsyncWebCrawler avec HTML Google Flights valide (10 vols) | Crawl URL ‚Üí Parse HTML | `len(flights) == 10`, tous Flight valid√©s Pydantic, pas d'exception |
| 2 | `test_integration_crawl_success_parse_zero_flights` | Mock AsyncWebCrawler avec HTML Google Flights vide (aucun `.pIav2d`) | Crawl URL ‚Üí Parse HTML vide | L√®ve `ParsingError("Zero valid flights extracted")`, crawl success mais parsing fail |

**Total tests int√©gration** : 2 tests

---

**TOTAL TESTS** : 23 unitaires + 2 int√©gration = **25 tests**

---

## Exemples JSON

**Exemple 1 : GoogleFlightDTO extrait et valid√©**
```json
{
  "price": 1270.0,
  "airline": "ANA",
  "departure_time": "10:30",
  "arrival_time": "14:45",
  "duration": "13 h 40 min",
  "stops": 1,
  "departure_airport": "Paris",
  "arrival_airport": "Tokyo"
}
```

**Note** : `departure_time` et `arrival_time` sont des `str` format HH:MM (heures locales sans date), pas `datetime` ISO 8601. Airports sont noms complets (max 200 car) ou codes IATA.

**Exemple 2 : Erreur CaptchaDetectedError**
```json
{
  "error": "CaptchaDetectedError",
  "message": "reCAPTCHA v2 detected after 3 retries",
  "details": {
    "url": "https://www.google.com/travel/flights?departure_id=CDG&arrival_id=NRT&outbound_date=2025-06-01",
    "captcha_type": "recaptcha_v2",
    "attempts": 3
  }
}
```

**Exemple 3 : ParsingError (aucun vol extrait)**
```json
{
  "error": "ParsingError",
  "message": "Zero valid flights extracted from HTML",
  "details": {
    "html_size": 123456,
    "base_selector_matches": 0,
    "reason": "No flight containers found in HTML"
  }
}
```

---

# üìä Observabilit√© & Monitoring

## Logging structur√©

Tous les logs doivent suivre le format JSON structur√© avec les champs suivants :

| Champ | Type | Description | Exemple |
|-------|------|-------------|---------|
| `timestamp` | ISO 8601 | Date et heure √©v√©nement | `"2025-11-19T10:30:45Z"` |
| `level` | String | Niveau log (DEBUG/INFO/WARNING/ERROR) | `"INFO"` |
| `service` | String | Service concern√© | `"CrawlerService"` |
| `message` | String | Message descriptif | `"Crawl successful"` |
| `url` | String | URL crawl√©e | URL Google Flights compl√®te |
| `status_code` | Integer | Code HTTP r√©ponse | `200`, `403`, `429` |
| `html_size` | Integer | Taille HTML en bytes | `245678` |
| `response_time_ms` | Integer | Temps r√©ponse en ms | `2345` |
| `stealth_mode` | Boolean | Stealth mode activ√© | `true` |

**Note** : Logs POC dev local (pas de proxy, pas de retry, stealth mode actif).

---

# ‚úÖ Crit√®res d'acceptation

## Crit√®res fonctionnels

1. **Crawl Google Flights 1 destination r√©ussi** : CrawlerService crawle avec succ√®s une URL Google Flights aller-simple (Paris‚ÜíTokyo) et retourne CrawlResult avec HTML valide (status 200, ‚â•100KB HTML)

2. **Stealth mode actif** : BrowserConfig configur√© avec `enable_stealth=True`, v√©rifi√© dans logs structur√©s (champ `stealth_mode: true`)

3. **Captcha d√©tect√© et logg√©** : Si reCAPTCHA/hCaptcha pr√©sent dans HTML ‚Üí CaptchaDetectedError lev√©e avec `captcha_type` logg√© en WARNING

4. **Parsing extrait minimum 5 vols** : FlightParser parse HTML Google Flights et retourne liste ‚â•5 Flight valid√©s Pydantic (champs obligatoires pr√©sents)

5. **Format Flight valide** : Chaque Flight contient `price > 0`, `airline` (2-100 caract√®res), `departure_time` et `arrival_time` (datetime ISO 8601), `duration` (format "Xh Ymin"), `stops` (int ‚â•0 ou None)

6. **Gestion champs manquants** : Si prix/compagnie/horaires absents ‚Üí vol skipp√© avec log WARNING, parsing continue pour vols suivants (pas d'exception bloquante)

7. **Erreurs explicites** : CaptchaDetectedError et ParsingError lev√©es avec messages descriptifs et contexte (URL, HTML size)

## Crit√®res techniques

8. **Type hints PEP 695** : Toutes signatures CrawlerService, FlightParser, Flight annot√©es avec type hints modernes (`list[Flight]`, `str | None`, `async def`)

9. **Async/Await coh√©rent** : CrawlerService.crawl_google_flights async, utilise `async with AsyncWebCrawler`, `await crawler.arun()`, pas de blocking IO

10. **Pydantic v2** : Flight utilise BaseModel avec Field pour d√©finir contraintes de validation, validation cross-champs pour coh√©rence temporelle, configuration strict (extra='forbid')

11. **JsonCssExtractionStrategy** : Configuration extraction CSS avec s√©lecteur de base pour identifier conteneurs vols, 8 champs minimum √† extraire (types text/attribute selon besoin), pas de LLM

12. **Logging structur√© JSON** : Tous logs incluent contexte m√©tier dans champs d√©di√©s : URL crawl√©e, code status HTTP, taille HTML re√ßu, temps de r√©ponse en millisecondes, activation stealth mode

13. **Exceptions custom** : CaptchaDetectedError et ParsingError h√©ritent de Exception standard Python, incluent attributs contextuels pour debugging : URL concern√©e, type captcha d√©tect√©, taille HTML, nombre vols trouv√©s

## Crit√®res qualit√©

14. **Coverage ‚â•80%** : Tests unitaires + int√©gration couvrent minimum 80% du code de CrawlerService et FlightParser (pytest-cov)

15. **21 tests passent** : 17 tests unitaires (7 CrawlerService + 10 FlightParser) + 4 tests int√©gration tous verts (pytest -v)

16. **Ruff + Mypy passent** : `ruff check .` et `ruff format .` sans erreur, `mypy app/` strict mode sans erreur type

17. **Tests TDD format AAA** : Tests unitaires suivent strictement Arrange/Act/Assert, tableaux specs compl√©t√©s avec 6 colonnes (N¬∞, Nom, Sc√©nario, Input, Output, V√©rification)

18. **Tests int√©gration format Given/When/Then** : Tests int√©gration suivent BDD avec 5 colonnes (N¬∞, Nom, Pr√©requis, Action, R√©sultat), mocks AsyncWebCrawler configur√©s

19. **Docstrings 1 ligne** : CrawlerService et FlightParser avec docstring descriptive, m√©thodes principales document√©es, focus POURQUOI pas QUOI

20. **Aucun code production dans specs** : Ce document contient uniquement signatures, tableaux tests, descriptions comportements, exemples JSON (pas d'impl√©mentation compl√®te de m√©thodes)

21. **Commits conventional** : Story 4 committ√©e avec message `docs(specs): add story 4 specifications` conforme Conventional Commits

---

**üí° Note** : Cette story est un Proof of Concept (8 story points). Les 21 crit√®res couvrent faisabilit√© technique (crawl + parsing 1 destination), robustesse (captcha detection, error handling), qualit√© (coverage, types, tests), et foundation r√©utilisable pour stories 5-7 (proxies, multi-city, retry logic production).
